---
layout: post
title:  "Python 음성데이터 분석 - MFCC (Mel Frequency Cepstral Coefficient)"
date:   22-12-20 00:30:00 +0530
categories: TimeSeries Librosa AudioData
---

지난 포스팅까지 소리의 특징부터 주파수 분석 및 Mel Scale까지 다양하게 살펴봤습니다. 이번 포스팅에서는 특히나 음성 분석에 많이 쓰이는 `Mel Frequency Cepstral Coefficient`에 대해 알아보겠습니다. 혹시나 잘못된 부분이 있으면 댓글 달아주시면 감사하겠습니다.

## MFCC (Mel Frequency Cepstral Coefficient)

mel spectrogram을 DCT(Discrete Cosine Transform) 처리하면 얻게되는 coefficient를 말합니다. 쉽게 얘기하면, mel scale로 변환한 스펙트로그램을 더 적은 값들로 압축하는 과정이라고 볼 수 있습니다. 이미지를 압축하는 과정에서도 DCT를 사용하기도 합니다.


## DCT (Discrete Cosine Transform)

DFT(Fourier) 대비 복소수 없이 실수로만 이루어져 있기 때문에 처리가 빨라 신호처리에서 사용한다고 합니다. 이론에 대한 빠삭한 이해보다는 제가 이해한 방식대로 설명을 해보도록 하겠습니다.


기본적으로 DCT는 코사인 변환입니다. 즉, 푸리에 변환과 마찬가지로 어떤 그래프(또는 이미지)가 있을 때, 그 그래프를 많은 코사인 함수로 표현한다고 볼 수 있습니다. 예를 들어 Mel-spectrogram에서 주파수 값이 총 1000개 였다면, 이를 그래프화한뒤 다시 주파수 분석을 하는 것입니다.  
다시 생각해보면, 원래 음원 데이터의 RAW 데이터는 파형이라고 볼 수 있습니다. 이 복잡한 파형을, sr이 22050이라고 한다면, 초당 22050개의 점을 찍어서 만든 아주 거대한 데이터입니다.이를 주파수 분석을 하면 초당 1000개로 표현할 수 있게 됩니다. 즉, 데이터 크기의 압축이 일어나게 되며, 그 과정에서 손실이 발생합니다.  
여기에서 그 주파수 분석된 결과를 또한번 주파수 분석한다고 이해해볼 수 있겠습니다. 그렇게 되면 원래 초당 22050개의 RAW데이터가 1000개로 축소되었는데, 이를 다시 20개로 축소할 수 있게 되는 것입니다.


DCT는 inverse 역변환이 가능하기 때문에, 결과적으로 20개의 값을 다시 RAW 데이터인 파형으로 돌릴 수 있습니다. 이렇게 데이터를 압축(손실압축)하는 방식이라고 볼 수 있습니다.



![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Phalaenopsis_fft_dct.png/220px-Phalaenopsis_fft_dct.png)  
*https://ko.wikipedia.org/wiki/이산\\\_코사인\\\_변환*



## 그렇게 압축된 데이터가 바로 MFCC, coefficient 입니다.

제가 이해한 것이 맞다면 원리는 설명한 바와 같습니다. 한번 더 깊게 생각해보면, 주파수 분석을 하게 되면 아주 작은 국소 구간에서의 값 편차는 DCT를 거칠 때 사라지게 될 것입니다. 코사인 함수의 주파수 단위로 분석하기 때문입니다. 이미지로 치면 비슷한 픽셀값들은 모두 동일한 값으로 처리되면서 압축이 될 것이며, 음원의 스펙트로그램으로 치면 비슷한 주파수 영역의 dB값은 모두 동일한 값으로 통일될 겁니다. 그렇게 데이터가 압축되고, 손실이 발생하는 구조입니다.



이 때 음원의 경우, `MFCC`를 거치면 노이즈가 제거된다고 볼 수도 있습니다. 사소하게 튀는 값들이 모두 DCT를 거치면서 압축/손실되기 때문입니다.


이렇게 MFCC를 통해 음원 혹은 음성 데이터의 주요 특징을 추출할 수 있다고 합니다. 추출된 요소가 바로 노이즈가 제거된 음원 데이터의 압축판이라고 볼 수 있지 않나 싶습니다.



## Librosa로 MFCC 구현하기

Librosa에서는 RAW데이터인 음원을 바로 MFCC로 만들어주기도 하고, 혹은 그 중간과정인 STFT와 Mel-Scale 값을 받아서 만들어주기도 합니다. 기능을 다양하게 제공하니 정말 잘 만든 라이브러리인 것 같습니다.

```python
D = np.abs(librosa.stft(y, n_fft = 2048, win_length = 2048, hop_length = 512))
mfcc = librosa.feature.mfcc(S = librosa.power_to_db(D), sr = sr, n_mfcc = 20)
```


![](https://i.stack.imgur.com/nvBcU.png)  
*https://stackoverflow.com/questions/60492462/mfcc-python-completely-different-result-from-librosa-vs-python-speech-features*



위와 같은 단순한 형태의 데이터를 얻을 수 있습니다. DCT를 거친 결과이기 때문에, 픽셀 관점이 아니라 주파수 관점에서 접근해야합니다. 즉 보이는 이미지가 특별한 의미가 있다기 보다는, 주파수가 어떻게 분포되는지가 더 중요하다고 볼 수 있는 것 같습니다. (특히 저주파에 대부분 몰려있는 것 같습니다.) 혼동이 되지만, 여기서 얘기하는 주파수는 DCT 결과물에 대한 것이지, 음원 자체의 FFT를 거쳐나온 주파수와는 다른 영역입니다.

---

## 결과적으로...

MFCC를 통해 음성 데이터를 분석하게 되면, 음원보다 상대적으로 갯수가 적은 coefficient들을 학습함으로써 보다 효율적으로 분석을 할 수 있게 됩니다.  
MFCC의 inverse 메소드들도 librosa에서 제공하고 있기 때문에, 역변환하는 과정만 후처리로 만들어준다면 음성데이터를 분석하는 전체적인 flow는 완성된다고 볼 수 있겠습니다.